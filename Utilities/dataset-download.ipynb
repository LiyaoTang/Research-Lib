{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import shutil\n",
    "import urllib\n",
    "import requests\n",
    "\n",
    "def get_files(root_dir, re_expr='.*'):\n",
    "    def _chk_re_expr(string):\n",
    "        return \n",
    "\n",
    "    file_list = []\n",
    "    for dir_n, _, fn_list in os.walk(root_dir):\n",
    "        if fn_list:\n",
    "            cur_fs = [os.path.join(dir_n, f) for f in fn_list]\n",
    "            cur_fs = [f for f in cur_fs if bool(re.fullmatch(re_expr, f))]\n",
    "            file_list += cur_fs\n",
    "    file_list = sorted(file_list)\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OTB-100\n",
    "from lxml import html, etree\n",
    "html = etree.parse(web, etree.HTMLParser())\n",
    "result = etree.tostring(html)\n",
    "result = html.xpath('//a[contains(@href, \"seq/\")]')\n",
    "\n",
    "base_url = 'http://cvlab.hanyang.ac.kr/tracker_benchmark/'\n",
    "folder = os.path.expanduser('~/Downloads/OTB100/')\n",
    "if os.path.isdir(folder):\n",
    "    shutil.rmtree(folder)\n",
    "os.makedirs(folder)\n",
    "for r in result:\n",
    "    url = base_url + r.attrib['href']\n",
    "    dst = os.path.join(folder, url.split('/')[-1])\n",
    "    \n",
    "    urllib.request.urlretrieve(url, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOT\n",
    "folder = os.path.expanduser('/Data/VOT/2018lt_zip/')\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "log = '/home/tangliyao/Research/vot-toolkit/vot-2018/log_2018lt'\n",
    "with open(log, 'r') as f:\n",
    "    log = f.read().split('\\n\\n')\n",
    "log = [l.strip('\\n ') for l in log if l.strip('\\n ')]\n",
    "\n",
    "skip_seq = set([])\n",
    "for seq in log:\n",
    "    seq = [i.strip('\\n ') for i in seq.split('\\n') if i.strip('\\n ')]\n",
    "    seq = [i for i in seq if '...' in i or 'url' in i]\n",
    "    \n",
    "    seq_n = seq[0].split('\"')[1]\n",
    "    ann_url = seq[1].split('=')[-1].strip()\n",
    "    img_url = seq[2].split('=')[-1].strip()\n",
    "    \n",
    "    if seq_n in skip_seq:\n",
    "        continue\n",
    "\n",
    "    print('downloading', seq_n)\n",
    "    dst_folder = os.path.join(folder, seq_n)\n",
    "    if os.path.isdir(dst_folder):\n",
    "        shutil.rmtree(dst_folder)\n",
    "    os.makedirs(dst_folder)\n",
    "    \n",
    "    for url in [ann_url, img_url]:\n",
    "        cnt = 0\n",
    "        dst = os.path.join(dst_folder, url.split('/')[-1])\n",
    "        print('  =>', dst)\n",
    "\n",
    "        while cnt < 100: # keep trying\n",
    "            try:\n",
    "                # urllib.request.urlretrieve(url, dst)\n",
    "                os.system('wget %s -O %s' % (url, dst))  # using 'wget'\n",
    "            except (urllib.error.URLError, urllib.error.HTTPError) as e:\n",
    "                err_msg = e.reason\n",
    "            except:\n",
    "                err_msg = sys.exc_info()[0]\n",
    "            else:  # success execution\n",
    "                time.sleep(1)\n",
    "                break\n",
    "            finally:  # clean-up actions\n",
    "                pass\n",
    "            # some error occured\n",
    "            print('\\t%s - retrying ...' % err_msg, cnt)\n",
    "            if os.path.isfile(dst):\n",
    "                os.remove(dst)\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check data\n",
    "import hashlib\n",
    "def get_md5(fname, chunk_size=1024*10):\n",
    "    chunk_size = int(chunk_size)\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(fname, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "data_dir = '/Data/VOT/2018lt_win'\n",
    "file_list = get_files(data_dir, '.*\\.zip')\n",
    "\n",
    "output_file = 'md5sum'\n",
    "with open(os.path.join(data_dir, output_file), 'w') as output_file:\n",
    "    for f in file_list:\n",
    "        md5 = get_md5(f)\n",
    "        output_file.write('%s: %s\\n' % (f, md5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncompress\n",
    "import zipfile\n",
    "data_dir = '/Data/VOT/2018lt_zip'\n",
    "file_list = get_files(data_dir, '.*\\.zip')\n",
    "\n",
    "for zip_f in file_list:\n",
    "    with zipfile.ZipFile(zip_f, 'r') as zip_ref:\n",
    "        target_dir = os.path.dirname(zip_f) # parent dir to uncrompress\n",
    "        target_dir = target_dir.replace('2018lt_zip', '2018lt')\n",
    "        \n",
    "        print(target_dir)\n",
    "        zip_ref.extractall(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}